{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# ---------------------------- 1. Load Data ----------------------------\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Replace 'your_data.csv' with the actual path to your data file\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Assuming the data is tab-separated as per the sample provided\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myour_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Display first few rows to verify\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial Data Sample:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mpjgl\\miniconda3\\envs\\seg3d\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mpjgl\\miniconda3\\envs\\seg3d\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\mpjgl\\miniconda3\\envs\\seg3d\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mpjgl\\miniconda3\\envs\\seg3d\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\mpjgl\\miniconda3\\envs\\seg3d\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_data.csv'"
     ]
    }
   ],
   "source": [
    "# Uncomment and run the following lines if you need to install any of these packages\n",
    "# !pip install pandas numpy scikit-learn imbalanced-learn matplotlib seaborn joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ---------------------------- 1. Load Data ----------------------------\n",
    "\n",
    "# Replace 'your_data.csv' with the actual path to your data file\n",
    "# Assuming the data is tab-separated as per the sample provided\n",
    "df = pd.read_excel(\"data/dataset.xlsx\", engine=\"openpyxl\")\n",
    "# Display first few rows to verify\n",
    "print(\"Initial Data Sample:\")\n",
    "display(df.head())\n",
    "\n",
    "# --------------------- 2. Handle Decimal Separators ---------------------\n",
    "\n",
    "# Identify numerical columns (excluding 'Num', 'Timestamp', 'Robot_ProtectiveStop', 'grip_lost')\n",
    "numerical_cols = df.columns.drop(['Num', 'Timestamp', 'Robot_ProtectiveStop', 'grip_lost'])\n",
    "\n",
    "# Replace commas with dots and convert to float\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].astype(str).str.replace(',', '.', regex=False).astype(float)\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"\\nData Types After Conversion:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# --------------------- 3. Encode Categorical Variables ---------------------\n",
    "\n",
    "bool_mapping = {False: 0, True: 1}\n",
    "df['Robot_ProtectiveStop'] = df['Robot_ProtectiveStop'].map(bool_mapping)\n",
    "df['grip_lost'] = df['grip_lost'].map(bool_mapping)\n",
    "\n",
    "# Verify the mapping\n",
    "print(\"\\nUnique values after encoding 'Robot_ProtectiveStop' and 'grip_lost':\")\n",
    "print(df[['Robot_ProtectiveStop', 'grip_lost']].nunique())\n",
    "\n",
    "# ------------------------- 4. Process Timestamp -------------------------\n",
    "\n",
    "# Convert 'Timestamp' to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# Extract useful time-based features\n",
    "df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['Minute'] = df['Timestamp'].dt.minute\n",
    "df['Second'] = df['Timestamp'].dt.second\n",
    "df['Microsecond'] = df['Timestamp'].dt.microsecond\n",
    "\n",
    "# Optionally, create a feature for elapsed time in seconds\n",
    "df = df.sort_values('Timestamp')  # Ensure data is sorted by time\n",
    "df['Elapsed_Time'] = (df['Timestamp'] - df['Timestamp'].min()).dt.total_seconds()\n",
    "\n",
    "# Drop the original 'Timestamp' column\n",
    "df = df.drop('Timestamp', axis=1)\n",
    "\n",
    "# Display the new features\n",
    "print(\"\\nData Sample After Timestamp Processing:\")\n",
    "display(df.head())\n",
    "\n",
    "# ----------------------- 5. Handle Missing Values -----------------------\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Handle missing values\n",
    "# For simplicity, we'll drop rows with any missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Verify that there are no missing values left\n",
    "print(\"\\nMissing Values After Dropping:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# ------------------------ 6. Feature Selection -------------------------\n",
    "\n",
    "# Define feature columns and target\n",
    "# Exclude 'Num' and 'grip_lost' from features\n",
    "feature_cols = df.columns.drop(['Num', 'grip_lost'])\n",
    "X = df[feature_cols]\n",
    "y = df['grip_lost']\n",
    "\n",
    "print(\"\\nFeature Columns:\")\n",
    "print(feature_cols.tolist())\n",
    "\n",
    "# -------------------- 7. Exploratory Data Analysis (EDA) ----------------\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(df[feature_cols].corr(), annot=False, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of target variable\n",
    "sns.countplot(x=y)\n",
    "plt.title('Distribution of grip_lost')\n",
    "plt.xlabel('Grip Lost (0 = False, 1 = True)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# If there's class imbalance, it will be evident from the count plot\n",
    "\n",
    "# ------------------------- 8. Split the Data ----------------------------\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTraining and Testing Set Sizes:\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# ------------------------ 9. Feature Scaling ----------------------------\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --------------------- 10. Handle Class Imbalance -----------------------\n",
    "\n",
    "# Check class distribution in training set\n",
    "print(\"\\nClass Distribution in y_train:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Initialize SMOTE for oversampling the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(\"\\nClass Distribution After SMOTE:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# ------------------------ 11. Model Training ----------------------------\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on resampled data\n",
    "rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# ------------------------ 12. Initial Evaluation ------------------------\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test_scaled)\n",
    "y_proba = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Initial Model):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Initial Model)')\n",
    "plt.show()\n",
    "\n",
    "# Print ROC AUC Score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC AUC Score (Initial Model): {roc_auc:.2f}\")\n",
    "\n",
    "# -------------------- 13. Hyperparameter Tuning -------------------------\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',  # Using F1-score for imbalanced data\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Perform grid search on resampled training data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest Parameters from GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Get the best estimator\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# --------------------- 14. Final Model Evaluation -----------------------\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_best = best_rf.predict(X_test_scaled)\n",
    "y_proba_best = best_rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Best Model):\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix_best = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_best, annot=True, fmt='d', cmap='Greens')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Best Model)')\n",
    "plt.show()\n",
    "\n",
    "# Print ROC AUC Score\n",
    "roc_auc_best = roc_auc_score(y_test, y_proba_best)\n",
    "print(f\"ROC AUC Score (Best Model): {roc_auc_best:.2f}\")\n",
    "\n",
    "# ----------------------- 15. Feature Importance --------------------------\n",
    "\n",
    "# Get feature importances from the best model\n",
    "importances = best_rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feat_importances = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# ------------------------- 16. Save the Model ----------------------------\n",
    "\n",
    "# Save the scaler and the best model using joblib\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(best_rf, 'random_forest_model.joblib')\n",
    "\n",
    "print(\"\\nModel and Scaler have been saved as 'random_forest_model.joblib' and 'scaler.joblib' respectively.\")\n",
    "\n",
    "# ------------------------ 17. Load and Predict (Optional) ----------------\n",
    "\n",
    "# Example of loading the saved model and scaler for future predictions\n",
    "# Uncomment the following lines to use them\n",
    "\n",
    "# loaded_scaler = joblib.load('scaler.joblib')\n",
    "# loaded_model = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "# # Example new data (ensure it has the same preprocessing)\n",
    "# # Replace 'new_data.csv' with your actual new data file path\n",
    "# new_df = pd.read_csv('new_data.csv', delimiter='\\t')\n",
    "\n",
    "# # Preprocess new data similarly\n",
    "# for col in numerical_cols:\n",
    "#     new_df[col] = new_df[col].astype(str).str.replace(',', '.', regex=False).astype(float)\n",
    "# new_df['Robot_ProtectiveStop'] = new_df['Robot_ProtectiveStop'].map(bool_mapping)\n",
    "# new_df['grip_lost'] = new_df['grip_lost'].map(bool_mapping)  # If available\n",
    "\n",
    "# # Process Timestamp if present\n",
    "# if 'Timestamp' in new_df.columns:\n",
    "#     new_df['Timestamp'] = pd.to_datetime(new_df['Timestamp'])\n",
    "#     new_df['Hour'] = new_df['Timestamp'].dt.hour\n",
    "#     new_df['Minute'] = new_df['Timestamp'].dt.minute\n",
    "#     new_df['Second'] = new_df['Timestamp'].dt.second\n",
    "#     new_df['Microsecond'] = new_df['Timestamp'].dt.microsecond\n",
    "#     new_df = new_df.sort_values('Timestamp')\n",
    "#     new_df['Elapsed_Time'] = (new_df['Timestamp'] - new_df['Timestamp'].min()).dt.total_seconds()\n",
    "#     new_df = new_df.drop('Timestamp', axis=1)\n",
    "\n",
    "# # Define features (ensure they match the training features)\n",
    "# new_X = new_df[feature_cols]\n",
    "\n",
    "# # Scale the new data\n",
    "# new_X_scaled = loaded_scaler.transform(new_X)\n",
    "\n",
    "# # Make predictions\n",
    "# new_predictions = loaded_model.predict(new_X_scaled)\n",
    "# new_probabilities = loaded_model.predict_proba(new_X_scaled)[:, 1]\n",
    "\n",
    "# print(\"\\nNew Predictions:\")\n",
    "# print(new_predictions)\n",
    "# print(\"\\nNew Prediction Probabilities:\")\n",
    "# print(new_probabilities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
